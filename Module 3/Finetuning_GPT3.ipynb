{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning GPT-3.5 Model"
      ],
      "metadata": {
        "id": "aKZs9cFNww9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxbPXfjKR2Fx"
      },
      "outputs": [],
      "source": [
        "# Step  1: Install the OpenAI Python library\n",
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lHncShqxLJN",
        "outputId": "9700134f-89ee-4614-d4b3-7d5561244286"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step  2: Import necessary libraries and set up the client\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "7JyQCeiOw1_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step  3: Prepare your dataset\n",
        "training_file_name = \"/content/gpt3_finetuning_data.jsonl\"\n",
        "\n",
        "# Upload training and validation files\n",
        "training_file_id = client.files.create(\n",
        "  file=open(training_file_name, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "print(f\"Training File ID: {training_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9coqNh-8xAuG",
        "outputId": "3a85a426-0b6e-40ac-ced5-8134d64552b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training File ID: FileObject(id='file-ZUPMOqgqMZUylhaarhDF25z5', bytes=56491, created_at=1708739046, filename='gpt3_finetuning_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_id.id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kZMNIb2bygow",
        "outputId": "aee4eeca-b56a-46b4-f077-45fab8acc589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-ZUPMOqgqMZUylhaarhDF25z5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step  4: Create a fine-tuning job\n",
        "\n",
        "# Adjust model parameters as needed\n",
        "job = client.fine_tuning.jobs.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  training_file=training_file_id.id,\n",
        ")\n",
        "\n",
        "print(job)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_hs4pXHyKZL",
        "outputId": "9e022ece-2c45-42c4-cd12-7873ac6d3e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-7fSzyJGgleBLXcNjZIP20yje', created_at=1708739152, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-rZBd7q5tFq4Amnr5IyUBPGbd', result_files=[], status='validating_files', trained_tokens=None, training_file='file-ZUPMOqgqMZUylhaarhDF25z5', validation_file=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# to be able to use LLM to generate text for the audio\n",
        "llm = ChatOpenAI(model = 'gpt-3.5-turbo-1106') # cost me aroud $1.5 for million tokens\n",
        "my_finetuned_llm = ChatOpenAI(model = \"ft:gpt-3.5-turbo-0613:personal::8vbX4gjJ\") # $0.5 + cost me aroud $6 for million tokens"
      ],
      "metadata": {
        "id": "VHioC2M9ybxR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "4df89e69-898c-43ac-9760-e329b2b382c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a567dbad9b6d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to be able to use LLM to generate text for the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-3.5-turbo-1106'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_finetuned_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ft:gpt-3.5-turbo-0613:personal::8vbX4gjJ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "siErUbDfCM6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}